{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "784_800_10.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jSG68G07feOc",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import time\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn as nn        \n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from IPython.display import Image, display\n",
        "import torch.multiprocessing as mp\n",
        "from torch.multiprocessing import Pool\n",
        "import Augmentor\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qoajoN72hjlW",
        "colab": {}
      },
      "source": [
        "MODELS = {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HBMe03cvfeOh",
        "colab": {}
      },
      "source": [
        "class Hyperparameters():\n",
        "    def __init__(self, train_bs, test_bs, lr, momentum, epochs):\n",
        "        super(Hyperparameters, self).__init__()\n",
        "        self.train_bs = train_bs\n",
        "        self.test_bs = test_bs\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.epochs = epochs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qC1iKueLfeOj",
        "colab": {}
      },
      "source": [
        "# defines linear classifier model \n",
        "# 400 -> 10 Linear Function -> log_softmax\n",
        "class LinearModel(nn.Module):\n",
        "    hyperparameters = Hyperparameters(100, 100, 0.01, 0.5, 5)\n",
        "    def __init__(self):\n",
        "        # define layers of net\n",
        "        super(LinearModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(400, 10)\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        # define the forward prop function\n",
        "        x = self.fc1(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "    \n",
        "    def preprocess(self, data, bs):\n",
        "        # preprocess input data\n",
        "        return data.view(bs, 400)\n",
        "      \n",
        "MODELS['Linear'] = LinearModel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PLyTKbwJkyb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvModel(nn.Module):\n",
        "  # hyperparameters = Hyperparameters(100, 100, 0.02, 0.9, 7) - 99\n",
        "    hyperparameters = Hyperparameters(150, 150, 0.01, 0.8, 14)\n",
        "    def __init__(self):\n",
        "        super(ConvModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1,6,3) # 1 input channel, 6 output channels, 3x3 conv kernel\n",
        "        self.conv2 = nn.Conv2d(6,12,3) # 6 input channels, 12 output channels, 3x3 conv kernel\n",
        "        self.fc1 = nn.Linear(300, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x)) # output -> [6,26,26]\n",
        "        x = F.max_pool2d(x, 2) # output -> [6,13,13]\n",
        "        \n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        \n",
        "        x = x.view(-1, self.num_flat_features(x)) # output -> [1, 1014]\n",
        "        x = F.relu(self.fc1(x)) # output -> [1, 500]\n",
        "        x = F.relu(self.fc2(x)) # output -> [1, 10]\n",
        "        return F.log_softmax(x, dim=1)\n",
        "    \n",
        "    def preprocess(self, data, bs):\n",
        "        return data\n",
        "                   \n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features\n",
        "\n",
        "MODELS['Conv'] = ConvModel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e33TIhKTfeOm",
        "colab": {}
      },
      "source": [
        "class HiddenModel(nn.Module):\n",
        "    hyperparameters = Hyperparameters(100, 100, 0.01, 0.6, 10)\n",
        "    def __init__(self):\n",
        "        super(HiddenModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 800)\n",
        "        self.fc2 = nn.Linear(800, 10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return F.log_softmax(x, dim=1)\n",
        "    \n",
        "    def preprocess(self, data, bs):\n",
        "        # preprocess input data\n",
        "        return data.view(bs, 784)\n",
        "      \n",
        "MODELS['Hidden'] = HiddenModel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tizOjN6cfeOt",
        "colab": {}
      },
      "source": [
        "# returns training data in tuple where \n",
        "# tuple[0] = X =  [1,20,20] tensor (20x20 input image)\n",
        "# tuple[1] = Y = [] tensor (Scalar output value)\n",
        "def get_train_data():\n",
        "    p = Augmentor.Pipeline('./data')\n",
        "    p.random_distortion(probability=1, grid_width=4, grid_height=4, magnitude=8)\n",
        "    p.sample(2)\n",
        "    transform = transforms.Compose([\n",
        "        p.torch_transform(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "    return datasets.MNIST('./data', \n",
        "                          train=True, \n",
        "                          transform=transform, \n",
        "                          target_transform=None, \n",
        "                          download=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Eu67GOm1feOw",
        "colab": {}
      },
      "source": [
        "# returns data batched according to specified batch size\n",
        "def batch_data(data, batch_size):\n",
        "    return torch.utils.data.DataLoader(\n",
        "        data, \n",
        "        pin_memory=True,\n",
        "        batch_size=batch_size, \n",
        "        shuffle=True)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_9edTbNefeOy",
        "colab": {}
      },
      "source": [
        "# returns testing data in tuple where \n",
        "# tuple[0] = X =  [1,20,20] tensor (20x20 input image)\n",
        "# tuple[1] = Y = [] tensor (Scalar output value)\n",
        "def get_test_data():\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "    return datasets.MNIST('./data',\n",
        "                          train=False,\n",
        "                          transform=transform,\n",
        "                          target_transform=None,\n",
        "                          download=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_RMi4Z5Jkyo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_one_example():\n",
        "    data = get_test_data()\n",
        "    return data[0][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YpGJXQn-feO1",
        "colab": {}
      },
      "source": [
        "def train(model, device, train_batches, optimizer, epoch):\n",
        "    model.train() # put model in training mode\n",
        "    \n",
        "    for batch_idx, (data, target) in enumerate(train_batches):\n",
        "        batch_size = len(data) # Question: Is this weird????\n",
        "        data = model.preprocess(data, batch_size) # reshape data to be 1 * 400\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target) # negative log likelihood loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "                \n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * batch_size, len(train_batches.dataset),\n",
        "                100. * batch_idx / len(train_batches), loss.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Wn1lF7CRfeO3",
        "colab": {}
      },
      "source": [
        "def test(model, device, test_batches):\n",
        "    #print (\"--- Testing ---\")\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(test_batches):\n",
        "            batch_size = len(data)\n",
        "            data = model.preprocess(data, batch_size)\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_batches.dataset)\n",
        "\n",
        "    print('\\nAverage loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_batches.dataset),\n",
        "        100. * correct / len(test_batches.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zL97rMZmW6oI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_epoch(model, device, train_batches, test_batches, optimizer, epoch):\n",
        "    train(model, device, train_batches, optimizer, epoch)\n",
        "    test(model, device, test_batches)\n",
        "    \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDtNoBx5dKYP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_hypers(bs_range, lr_range):\n",
        "  pairs = []\n",
        "  for bs in bs_range:\n",
        "    for lr in lr_range:\n",
        "      pairs.append({'lr': lr, 'bs': bs})\n",
        "  return pairs\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqHjiSmCbl10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def grid_search():\n",
        "  \n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\") if use_cuda else torch.device(\"cpu\")\n",
        "    print(\"device: \"+ torch.cuda.get_device_name())\n",
        "    model_name = 'Conv'\n",
        "\n",
        "    train_data = get_train_data()\n",
        "    test_data = get_test_data()\n",
        "\n",
        "    bs_range = [100, 110, 120, 130, 140, 150]\n",
        "    lr_range = [0.01]\n",
        "    pairs = get_hypers(bs_range, lr_range)\n",
        "\n",
        "    for pair in pairs:\n",
        "        model = MODELS[model_name]()\n",
        "        hyperparams = model.hyperparameters\n",
        "        train_bs = pair['bs'] # training batch size\n",
        "        test_bs = pair['bs'] # test batch size\n",
        "        lr = pair['lr'] # learning rate\n",
        "        momentum = hyperparams.momentum # momentum ??\n",
        "        epochs = hyperparams.epochs # epochs\n",
        "        \n",
        "        train_batches = batch_data(train_data, train_bs)\n",
        "        test_batches = batch_data(test_data, test_bs)\n",
        "        \n",
        "        \n",
        "        model = model.to(device) # load model to cpu\n",
        "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "        \n",
        "        for epoch in range(1, epochs + 1):\n",
        "            train(model, device, train_batches, optimizer, epoch)\n",
        "        \n",
        "        print(\"Testing bs: %s\\tlr: %s\" % (pair['bs'], pair['lr']))\n",
        "        test(model, device, test_batches)\n",
        "    \n",
        "    print('Done!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC0cFoySct28",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_search()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8mFiHObefeO6",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\") if use_cuda else torch.device(\"cpu\")\n",
        "    print(\"device: \"+ torch.cuda.get_device_name())\n",
        "    model_name = 'Conv'\n",
        "    model = MODELS[model_name]()\n",
        "    \n",
        "    hyperparams = model.hyperparameters\n",
        "    train_bs = hyperparams.train_bs # training batch size\n",
        "    test_bs = hyperparams.test_bs # test batch size\n",
        "    lr = hyperparams.lr # learning rate\n",
        "    momentum = hyperparams.momentum # momentum ??\n",
        "    epochs = hyperparams.epochs # epochs\n",
        "    \n",
        "    \n",
        "    train_data = get_train_data()\n",
        "    train_batches = batch_data(train_data, train_bs)\n",
        "    \n",
        "    test_data = get_test_data()\n",
        "    test_batches = batch_data(test_data, test_bs)\n",
        "    \n",
        "    \n",
        "    model = model.to(device) # load model to cpu\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "\n",
        "    start = time.perf_counter()\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train(model, device, train_batches, optimizer, epoch)\n",
        "        test(model, device, test_batches)\n",
        "    end = time.perf_counter()\n",
        "    print('Done!: ' + str(end-start))\n",
        "\n",
        "    #torch.save(model, './linearclassifier.pth')\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}